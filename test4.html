<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Voice-Controlled Box Change</title>
  <style>
    body {
      margin: 0;
      padding: 0;
      height: 100vh;
      display: flex;
      justify-content: center;
      align-items: center;
      flex-direction: column;
      background-color: #f0f0f0;
    }
    .square {
      width: 100px;
      height: 100px;
      position: absolute;
      background-color: #3498db;  /* Default color */
    }
    .slider-container {
      margin-top: 20px;
      text-align: center;
    }
  </style>
</head>
<body>

  <div id="square" class="square"></div>

  <div class="slider-container">
    <label for="thresholdSlider">Threshold: </label>
    <input type="range" id="thresholdSlider" min="0" max="1" step="0.01" value="0.2">
    <span id="thresholdValue">0.2</span>
  </div>

  <script>
    const square = document.getElementById('square');
    const thresholdSlider = document.getElementById('thresholdSlider');
    const thresholdValue = document.getElementById('thresholdValue');

    let threshold = parseFloat(thresholdSlider.value);  // Initial threshold

    // Set up Web Audio API
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
    const analyser = audioContext.createAnalyser();
    analyser.fftSize = 256;  // Set analyzer FFT size (this defines frequency resolution)
    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    // Wait for user interaction to start the audio context
    document.body.addEventListener('click', () => {
      if (audioContext.state === 'suspended') {
        audioContext.resume();
        console.log("AudioContext resumed");
      }
    });

    // Request access to the microphone
    navigator.mediaDevices.getUserMedia({ audio: true })
      .then(stream => {
        console.log("Microphone access granted.");
        const source = audioContext.createMediaStreamSource(stream);
        source.connect(analyser);
        monitorAudio();
      })
      .catch(error => {
        console.error('Error accessing microphone:', error);
      });

    // Function to monitor audio levels and change the color of the square based on volume
    function monitorAudio() {
      analyser.getByteTimeDomainData(dataArray);  // Get waveform data from the audio stream

      let sum = 0;
      // Calculate the average loudness by checking the amplitude of the waveform
      for (let i = 0; i < bufferLength; i++) {
        sum += Math.abs(dataArray[i] - 128);  // Get the absolute difference from the midpoint (128)
      }

      // Normalize the loudness (average value) to a range of 0 to 1
      const averageLoudness = sum / bufferLength;
      const normalizedLoudness = averageLoudness / 128; // Normalize to a 0-1 scale (128 is the middle value)

      // Log the loudness value for debugging
      console.log("Loudness:", normalizedLoudness);

      // Check if the loudness exceeds the threshold
      if (normalizedLoudness > threshold) {
        // Change the color to red if the loudness is above the threshold
        square.style.backgroundColor = '#e74c3c';  // Red color for high sound
      } else {
        // Change back to blue when the loudness is below the threshold
        square.style.backgroundColor = '#3498db';  // Blue color for quiet sound
      }

      // Continue monitoring the audio
      requestAnimationFrame(monitorAudio);
    }

    // Update the threshold value dynamically when the slider changes
    thresholdSlider.addEventListener('input', (event) => {
      threshold = parseFloat(event.target.value);  // Update threshold value
      thresholdValue.textContent = threshold.toFixed(2);  // Show the updated threshold
    });

  </script>

</body>
</html>
